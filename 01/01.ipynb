{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f7207-e66c-4701-8756-f81a02151ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11526d2-d249-4b0e-b0ee-a6d3a076ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '../data/domain_names_full.txt'\n",
    "dataset = open(dataset_name, 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da4232-71c1-4e3c-b645-ccee5408770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8eb5cb-9c34-42e7-ae33-3173e0c7aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(x) for x in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e48795-ee4b-437a-9fb6-02093bb3e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([len(x) for x in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecedcb3-150d-4acf-b7c5-a16d8c51002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[19484]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6132876-2f8d-4a5a-b7b5-a6f425653239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset[:10]:\n",
    "    for c1, c2 in zip(d, d[1:]):\n",
    "        print(c1,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2bc1f-9d70-4901-8aaa-3f45aeee8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'*' in set([y for x in dataset for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c144d-906b-49b3-b42e-fe59f0aa97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the bigrams for each example\n",
    "for d in dataset[:1]:\n",
    "    example = ['*'] + list(d) + ['*']\n",
    "    for c1, c2 in zip(example, example[1:]):\n",
    "        print(c1,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36738f32-55eb-4889-bf0d-c0185014febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is expensive... \n",
    "bigrams = dict()\n",
    "for d in dataset:\n",
    "    example = ['*'] + list(d) + ['*']\n",
    "    for c1, c2 in zip(example, example[1:]):\n",
    "        bigrams[(c1, c2)] = bigrams.get((c1,c2), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61857dd-cc08-4738-957c-325593a5e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigrams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80923717-b035-4e05-82a6-0a6a19d7a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = ['*'] + sorted(list(set([y for x in dataset for y in x])))\n",
    "ctoi = {c:i for i, c in enumerate(charset)}\n",
    "itoc = {i:c for i, c in enumerate(charset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937eab9-1d55-4269-9426-eced4c5c6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae5c59-881b-410d-8e45-f166be1fa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bigram_count = np.zeros((5, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3327c9-5843-4f1e-a34a-87fa49ff82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count = np.zeros((len(charset), len(charset))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae8307-fa2c-4e73-9c74-b6269cdf9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2369474-c09c-4adc-ad1c-a052e62135ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:  # <== remove [:10] to run for all examples\n",
    "    example = ['*'] + list(d) + ['*']\n",
    "    for c1, c2 in zip(example, example[1:]):\n",
    "        bigram_count[ctoi[c1], ctoi[c2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4b1e1-28f1-426e-8cbe-b365e8fc6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e1bf5-3157-49a0-b534-15445a3a77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(bigram_count, cmap='Blues')\n",
    "for i in range(len(charset)):\n",
    "    for j in range(len(charset)):\n",
    "        chars = itoc[i]+itoc[j]\n",
    "        plt.text(i, j, chars,ha='center', va='bottom', color='grey', fontsize=8)\n",
    "        plt.text(i, j, bigram_count[i,j],ha='center', va='top', color='grey', fontsize=8)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577d86b-ec40-4aca-be7d-fc7fa7ee1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bigram_count[0]\n",
    "p = p/p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28743d7-5cf0-434b-90cd-7181f22c7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5173a0-d942-4930-a083-aa49ffa7da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = np.random.choice(charset, 1, p=p, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8b8be-3f98-409d-940f-0632f5dcc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154f512-7d82-4ae1-ba8d-4b599bdea8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c9d13-ad97-490d-821a-b7417b8cf4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(bigram_count):\n",
    "    new = []\n",
    "    p = bigram_count[0]\n",
    "    p = p/p.sum()\n",
    "    draw = np.random.choice(charset, 1, p=p, replace=True)[0]\n",
    "    while draw != '*':\n",
    "        p = bigram_count[ctoi[draw]]\n",
    "        p = p/p.sum()\n",
    "        draw = np.random.choice(charset, 1, p=p, replace=True)[0]\n",
    "        new.append(draw)\n",
    "    return ''.join(new[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ccd24-296f-41a0-84c4-bcb38e094c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate(bigram_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23b59d-8487-4d8e-b31d-f0997067b05a",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood\n",
    "Las probabilidades que el modelo asigna a cada bigrama individual debería ser lo más cercanas a 1 posible. Para cada bigrama, si la probabilidad fuera uniforme, tendríamos una probabilidad de `1/len(charset)`. Cualquier modelo que aprenda algo asignará a algunos bigramas valores más altos de probabilidad. Si nuestro modelo fuera perfecto, asignaría probabilidad 1 a cada uno de los bigramas individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cde3dd-7a8b-49ba-a346-7e1a336b8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/len(charset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a95d4-73df-4051-ad2e-2b5fa1e9fac8",
   "metadata": {},
   "source": [
    "El estimador de máxima verosimilitud (maximum likelihood estimation) se define como el producto de las probabilidades. Vamos a usar este número como indicador de cuan bueno es nuestro modelo. Así, si el estimador es 1 el modelo es perfecto. Sin embargo, noten que producto de valores entre 0 y 1 generalmente está mal condicionado numéricamente, es decir, da números muy chicos, más chicos que los límites de representación de números en punto flotante y se termina redondeando a cero. Para evitar esto se usa el logaritmo del likelihood. Además, recuerden que $$log(a*b*c) = log(a)+(log(b)+log(c)$$ \n",
    "Por lo tanto podemos calcular el log likelihood de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f0cba-0f63-48c0-8878-b040a806f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = bigram_count/bigram_count.sum(axis=1, keepdims=True)  # Calculamos la matriz de probabilidades (en lugar de matriz de cuentas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4846d41a-b91a-4463-8bd3-ebf820dc35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "for d in dataset[:2]:\n",
    "    example = ['*'] + list(d) + ['*']\n",
    "    for c1, c2 in zip(example, example[1:]):\n",
    "        prob = bigram_prob[ctoi[c1], ctoi[c2]]\n",
    "        log_likelihood += np.log(prob)\n",
    "        print(f'{c1}{c2} {prob:.4f}')\n",
    "print(f'{log_likelihood=:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45950b-03fd-486d-82e3-b2be02177f0a",
   "metadata": {},
   "source": [
    "Para hacer comparaciones entre modelos y datasets, lo mejor normalizar el (log) likelihood por la cantidad de muestras utilizada para calcularlo. Así, usamos el promedio del (log) likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b351994-f959-4f21-a66c-515b72abad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodprobs = 1\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for d in dataset[:2]:\n",
    "    example = ['*'] + list(d) + ['*']\n",
    "    for c1, c2 in zip(example, example[1:]):\n",
    "        prob = bigram_prob[ctoi[c1], ctoi[c2]]\n",
    "        prodprobs *= prob\n",
    "        log_likelihood += np.log(prob)\n",
    "        n += 1\n",
    "        print(f'{c1}{c2} {prob:.4f}')\n",
    "print(f'{prodprobs=} <== Noten que pequeño este número')  # <== Noten que pequeño este número\n",
    "print(f'{log_likelihood= :0.4f}')\n",
    "print(f'Average log_likelihood: {log_likelihood/n:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d1256-9307-44e1-815d-1e9d441ef929",
   "metadata": {},
   "source": [
    "Por ultimo, vamos a usar esto como función de perdida (loss function). La semántica de la loss function indica que cuanto menor es el valor, mejor es el modelo. Por lo tanto, usamos el negative log likelihood. Como el logaritmo es una función monotónica creciente, que para las probabilidades va desde $-\\infty$ hasta 0, el negativo de la función es monotónicamente descreciente desde $\\infty$ hasta 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44497b78-27b4-4427-9c03-44a2a250bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,1.001, 0.001)\n",
    "plt.plot(np.log(x), label='log(x)')\n",
    "plt.plot(-np.log(x), label='-log(x)')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba555137-b978-446f-ad0f-f0fcee99f7af",
   "metadata": {},
   "source": [
    "Se cumplen entonces las siguientes equicalencias:\n",
    "- Maximizar el Likelihood es equivalente a:\n",
    "- Maximizar el Log Likelihood que es equivalente a:\n",
    "- Maximizar el Promedio del Log Likelihood que es equivalente a:\n",
    "- Minimizar el Negativo del Promedio del Log Likelihood\n",
    "\n",
    "Por lo tanto, podemos usar el negative log likelihood promedio como función de perdida y para evaluar que tan buenos son nuestros modelos. Esta función es la misma que usaremos para entrenar desde los modelos de redes neuronales hasta los transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b476fbf-859e-45cc-b626-c8088fb1a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(s: list[str]) -> float:\n",
    "    log_likelihood = 0.\n",
    "    n = 0\n",
    "    for d in s:\n",
    "        example = ['*'] + list(d) + ['*']\n",
    "        for c1, c2 in zip(example, example[1:]):\n",
    "            prob = bigram_prob[ctoi[c1], ctoi[c2]]\n",
    "            log_likelihood += np.log(prob)\n",
    "            n += 1\n",
    "            print(f'{c1}{c2} {prob:.4f} {np.log(prob):0.4f}')\n",
    "    return -log_likelihood/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca6503-4e49-488c-9112-5c93eab8f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll(['joaquin.cmm*']) # <-- Por esto necesitamos model smoothing.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e81f13-05b7-49fe-9ab0-9ddd357f7c86",
   "metadata": {},
   "source": [
    "# Model Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da70313-090a-4fef-929e-66356c183fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis2]",
   "language": "python",
   "name": "conda-env-analysis2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

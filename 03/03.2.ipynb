{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac94534-4296-4baa-aa91-158b7f002147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d80ffb-3666-4de7-b800-9bada8758d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrimos el archivo con los nombres\n",
    "dataset = open('../data/domain_names_full.txt', 'r').read().splitlines()[:10000] # Usamos solo el primer millon de dominios\n",
    "dataset[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e768c-30f0-4fed-93dc-f480d72dbb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = ['*'] + sorted(list(set([y for x in dataset for y in x])))\n",
    "ctoi = {c:i for i, c in enumerate(charset)}\n",
    "itoc = {i:c for i, c in enumerate(charset)}\n",
    "charset_len = len(charset)\n",
    "print(ctoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15170998-ece1-4646-9ed4-2dd9cde3f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset: list):\n",
    "    X, Y  = [], []\n",
    "    for d in dataset:\n",
    "        example = list(d) + ['*']\n",
    "        context = [0] * context_size\n",
    "        for c in example:\n",
    "            X.append(context)\n",
    "            Y.append(ctoi[c])\n",
    "            context = context[1:] + [ctoi[c]] \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc506c70-4cf0-485b-a9e6-4da95539efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "context_size = 3\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(dataset)\n",
    "n1 = int(.8 * len(dataset))  # límite para el 80% del dataset\n",
    "n2 = int(.9 * len(dataset))  # límite para el 90% del dataset\n",
    "Xtr, Ytr = build_dataset(dataset[:n1])    # 80%\n",
    "Xva, Yva = build_dataset(dataset[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(dataset[:n2])    # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830677c-f778-4642-bafb-f07df2be4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP de nuevo...\n",
    "g = torch.Generator(device='cpu').manual_seed(42)\n",
    "emb_d = 10  # El número de dimensiones del Enbedding\n",
    "input_size = context_size * emb_d  # el tamaño del input, desapilado\n",
    "n_hidden = 128  # El número de neuronas en al capa hiddend que queremos\n",
    "\n",
    "# Definición del modelo\n",
    "C = torch.randn((charset_len, emb_d),             generator=g)\n",
    "W1 = torch.randn((emb_d * context_size, n_hidden),generator=g)\n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, charset_len),         generator=g)\n",
    "b2 = torch.randn(charset_len,                     generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572df746-bda0-4894-9fcf-519541f68bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations = 200000\n",
    "minibatch_size = 64\n",
    "loss_log = []\n",
    "\n",
    "for i in range(train_iterations):\n",
    "    # training loop en mini batches\n",
    "    ix = torch.randint(0, len(Xtr), (minibatch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    # forward pass usando F.cross_entropy\n",
    "    emb = C[Xb]  # embedding de los caracteres\n",
    "    embcat = emb.view(-1, input_size)  # embedding como una vector de input_size\n",
    "    hpreact = embcat @ W1 + b1  # pre activación de la capa oculta (h)\n",
    "    h = torch.tanh(hpreact)  # activación de la capa oculta (h)\n",
    "    logits = h @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    lr = .1 if i < 100000 else .01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    if i % 10000 == 0:\n",
    "        print(f'Step: {i:7d}/{train_iterations:7d} -- loss: {loss.item():.6f}')\n",
    "    loss_log.append(loss.log10().item())\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78cca2-6d8a-4d84-80d3-760e11e80803",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b46b78-2804-4ae7-b6c5-014a9e6782bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d88451-c031-495c-984f-f598ee076b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(h.abs() > 0.99, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5f56d-ddb9-44ee-87e6-c59e5920b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self, n_embeddings, embedding_dimension):\n",
    "        self.weight = torch.randn((n_embeddings, embedding_dimension))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = self.weight[x]\n",
    "        self.out = self.out.view(self.out.shape[0], -1)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out))\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06edf4-3867-4c36-869f-03356e72e525",
   "metadata": {},
   "source": [
    "## A practicar!\n",
    "El siguiente código tiene algunos errores. En pricipio, el loop de training no aprende nada (el loss no baja) más allá del segundo loop. Encuentren el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8385c-81c4-4b5a-a0f2-a8263b181212",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "emb_d = 10\n",
    "n_hidden = 128\n",
    "\n",
    "model = [\n",
    "    Embedding(charset_len, emb_d),\n",
    "    Linear(emb_d*context_size, n_hidden), Tanh(),\n",
    "    Linear(n_hidden, charset_len)\n",
    "]\n",
    "\n",
    "parameters = [p for layer in model for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(f'Number of parameters: {sum((p.nelement() for p in parameters))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfee399-f95d-418b-bf5e-c669057a1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations = 1000000\n",
    "minibatch_size = 64\n",
    "loss_log = []\n",
    "\n",
    "for i in range(train_iterations):\n",
    "    # training loop en mini batches\n",
    "    ix = torch.randint(0, len(Xtr), (minibatch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    x = Xb\n",
    "    # forward pass\n",
    "    for layer in model:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for layer in model:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data *= -lr * p.grad\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(f'Step: {i:7d}/{train_iterations:7d} -- loss: {loss.item():.6f}')\n",
    "    loss_log.append(loss.log10().item())\n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179424a0-fa89-4934-bc33-31ec31b06d79",
   "metadata": {},
   "source": [
    "### Haciendo inferencia\n",
    "Una vez entrenado el modelo, hay que escribir una función para samplear del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd5fff-3c8f-422e-8e0c-61c60368442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para samplear del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaef89-d90f-40f2-9e03-187f91f6cb53",
   "metadata": {},
   "source": [
    "### Agregando capas\n",
    "Nuestro modelo tiene una capa sola pero ahora agregar capas es re facil!! \n",
    "\n",
    "El siguiente modelo tiene varias capas. Analicen como cambiarían el training loop y la función de inferencia para que el modelo siga funcionando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ae425-4078-4b49-a172-7ea9eab57591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\n",
    "    Embedding(charset_len, emb_d),\n",
    "    Linear(emb_d*context_size, n_hidden), Tanh(),\n",
    "    Linear(emb_d*context_size, n_hidden), Tanh(),\n",
    "    Linear(emb_d*context_size, n_hidden), Tanh(),\n",
    "    Linear(emb_d*context_size, n_hidden), Tanh(),\n",
    "    Linear(n_hidden, charset_len)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c30b0-3c9d-4f45-adfa-1ff4178222d7",
   "metadata": {},
   "source": [
    "### Diagnóstico y análisis\n",
    "Estaría bueno poder mostrar la saturación de la función de activación y algunos otros plots de diagnostico.\n",
    "\n",
    "Modifiquen el modelo anterior para que tenga 7 capas ocultas. Analicen que pasa con las funciones de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c378ff7c-ca11-49b1-8c67-359c918ac00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59834c60-5f34-4007-980a-b99a2dde1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturación de las neuronas de las diferentes capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "721d7658-bea9-406e-a243-eabdb11e6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de saturacion de las capas (a la Karpathy https://youtu.be/P6sfmUTpUmc?feature=shared&t=5242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3be68-8dbc-44bc-b93a-9b2db0fb3269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis2]",
   "language": "python",
   "name": "conda-env-analysis2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
